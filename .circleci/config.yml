version: 2
jobs:
  test_backend:
    docker:
      - image: cpdbdev/backend:latest
        user: gunicorn
      - image: cpdbdev/postgres:9.6
      - image: elasticsearch:5-alpine
        name: elasticsearch
      - image: selenium/standalone-chrome:3.11
    environment:
      DJANGO_SETTINGS_MODULE: config.settings.circleci
      DB_HOST: localhost
      DB_USER: cpdb
      DB_PASSWORD: password
      DB_NAME: cpdb
    steps:
      - checkout
      - run: pip install --user -r requirements/test.txt
      - run: flake8
      - run: coverage run cpdb/manage.py test --noinput --nologcapture
      - run: coverage combine
      - run: coverage report --omit="/home/ubuntu/virtualenvs/*"
      - run: coveralls
      - store_artifacts:
          path: /usr/src/app/project/cpdb/test_visual_token_media

  push_backend_image:
    machine: true
    environment:
      ANSIBLE_HOST_KEY_CHECKING: "False"
    steps:
      - checkout
      - run: echo "build-$CIRCLE_BUILD_NUM" > buildnum
      - persist_to_workspace:
          root: .
          paths:
            - buildnum
      - run:
          name: Build image
          command: docker build -t cpdbdev/backend:$(cat buildnum)
      - run:
          name: Push image
          command: docker push cpdbdev/backend:$(cat buildnum)

  django_collect_static:
    docker:
      - image: cpdbdev/kubectl:v1.11.3
    steps:
      - checkout
      - attach_workspace:
          at: /tmp/workspace
      - run:
          name: Reveal secrets
          command: |
            echo $GPG_PRIVATE_KEY | base64 -d > /tmp/gpg_private.gpg
            gpg --allow-secret-key-import --import /tmp/gpg_private.gpg
            git secret reveal
      - run: echo $KUBECONFIG_JSON > /home/kubectl/.kube/config
      - run:
          name: Start job
          command: |
            if [ $CIRCLE_BRANCH == "master" ]
            then
              export CONTEXT=production
            else
              export CONTEXT=staging
            fi
            bin/run_job.sh --$CONTEXT django_collect_static.yml django-collect-static $(cat /tmp/workspace/buildnum)

  django_migrate:
    docker:
      - image: cpdbdev/kubectl:v1.11.3
    steps:
      - checkout
      - attach_workspace:
          at: /tmp/workspace
      - run:
          name: Reveal secrets
          command: |
            echo $GPG_PRIVATE_KEY | base64 -d > /tmp/gpg_private.gpg
            gpg --allow-secret-key-import --import /tmp/gpg_private.gpg
            git secret reveal
      - run: echo $KUBECONFIG_JSON > /home/kubectl/.kube/config
      - run:
          name: Start job
          command: |
            if [ $CIRCLE_BRANCH == "master" ]
            then
              export CONTEXT=production
            else
              export CONTEXT=staging
            fi
            bin/run_job.sh --$CONTEXT django_migrate.yml django-migrate $(cat /tmp/workspace/buildnum)

  rebuild_index:
    docker:
      - image: cpdbdev/kubectl:v1.11.3
    steps:
      - checkout
      - attach_workspace:
          at: /tmp/workspace
      - run:
          name: Reveal secrets
          command: |
            echo $GPG_PRIVATE_KEY | base64 -d > /tmp/gpg_private.gpg
            gpg --allow-secret-key-import --import /tmp/gpg_private.gpg
            git secret reveal
      - run: echo $KUBECONFIG_JSON > /home/kubectl/.kube/config
      - run:
          name: Start job
          command: bin/run_job.sh --production rebuild_index.yml rebuild-index $(cat /tmp/workspace/buildnum)

  rebuild_search_index:
    docker:
      - image: cpdbdev/kubectl:v1.11.3
    steps:
      - checkout
      - attach_workspace:
          at: /tmp/workspace
      - run:
          name: Reveal secrets
          command: |
            echo $GPG_PRIVATE_KEY | base64 -d > /tmp/gpg_private.gpg
            gpg --allow-secret-key-import --import /tmp/gpg_private.gpg
            git secret reveal
      - run: echo $KUBECONFIG_JSON > /home/kubectl/.kube/config
      - run:
          name: Start job
          command: bin/run_job.sh --production rebuild_search_index.yml rebuild-search-index $(cat /tmp/workspace/buildnum)

  deploy_backend:
    docker:
      - image: cpdbdev/kubectl:v1.11.3
    steps:
      - checkout
      - attach_workspace:
          at: /tmp/workspace
      - run:
          name: Reveal secrets
          command: |
            echo $GPG_PRIVATE_KEY | base64 -d > /tmp/gpg_private.gpg
            gpg --allow-secret-key-import --import /tmp/gpg_private.gpg
            git secret reveal
      - run: echo $KUBECONFIG_JSON > /home/kubectl/.kube/config
      - run:
          name: Apply deployment
          command: |
            if [ $CIRCLE_BRANCH == "master" ]
            then
              export NAMESPACE=production
              source prod.env
            else
              export NAMESPACE=staging
              source staging.env
            fi
            BACKEND_IMAGE_TAG=$(cat /tmp/workspace/buildnum) templater kubernetes/gunicorn.yml | kubectl apply -f - --namespace=$NAMESPACE

  build_cpdpbot:
    machine: true
    steps:
      - checkout
      - run: echo "build-$CIRCLE_BUILD_NUM" > buildnum
      - persist_to_workspace:
          root: .
          paths:
            - buildnum
      - run:
          name: Build cpdpbot image
          command: docker build -t cpdbdev/cpdpbot:$(cat buildnum) docker/cpdpbot
      - run:
          name: Test cpdpbot image
          command: docker run -e "SETUP_LOGGING=no" --rm cpdbdev/cpdpbot:$(cat buildnum) python -m cpdpbot.test
      - run:
          name: Push cpdpbot image
          command: |
            echo $DOCKER_PASS | docker login -u $DOCKER_USER --password-stdin
            docker push cpdbdev/cpdpbot:$(cat buildnum)

  deploy_cpdpbot:
    docker:
      - image: cpdbdev/kubectl:v1.11.3
    steps:
      - checkout
      - attach_workspace:
          at: /tmp/workspace
      - run: echo $KUBECONFIG_JSON > /home/kubectl/.kube/config
      - run:
          name: Reveal secrets
          command: |
            echo $GPG_PRIVATE_KEY | base64 -d > /tmp/gpg_private.gpg
            gpg --allow-secret-key-import --import /tmp/gpg_private.gpg
            git secret reveal
      - run:
          name: Deploy cpdpbot
          command: CPDPBOT_IMAGE_TAG=$(cat /tmp/workspace/buildnum) templater kubernetes/cpdpbot.yml -f prod.env | kubectl apply --namespace=production -f -

  deploy_ingress:
    docker:
      - image: cpdbdev/kubectl:v1.11.3
    steps:
      - checkout
      - run: echo $KUBECONFIG_JSON > /home/kubectl/.kube/config
      - run:
          name: Reveal secrets
          command: |
            echo $GPG_PRIVATE_KEY | base64 -d > /tmp/gpg_private.gpg
            gpg --allow-secret-key-import --import /tmp/gpg_private.gpg
            git secret reveal
      - run:
          name: Install nginx ingress controller
          command: |
            kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml
            kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/provider/cloud-generic.yaml
      - run:
          name: Apply ingress
          command: |
            if [ $CIRCLE_BRANCH == "master" ]
            then
              export NAMESPACE=production
              export ENV_FILE=prod.env
            else
              export NAMESPACE=staging
              export ENV_FILE=staging.env
            fi
            set +e
            kubectl apply -f kubernetes/namespaces.yml
            kubectl create secret tls tls-secret --key tls_key --cert tls_cert --namespace=$NAMESPACE || true
            templater kubernetes/ingress.yml -f $ENV_FILE | kubectl apply --namespace=$NAMESPACE -f -

  create_sql_dump_file:
    docker:
      - image: cpdbdev/postgres:9.6
    steps:
      - run:
          name: Create SQL Dump file
          command: |
            PGPASSWORD=$POSTGRES_APP_PASSWORD pg_dump \
              --username $POSTGRES_APP_USER \
              --host="$POSTGRES_HOST" \
              --port=5432 \
              "sslmode=require dbname=$POSTGRES_APP_DB" > cpdb_production_dump.sql.gz
      - persist_to_workspace:
          root: .
          paths:
            - cpdb_production_dump.sql.gz

  upload_sql_file_to_azure:
    docker:
      - image: emeraldsquad/azcopy
    steps:
      - attach_workspace:
          at: /tmp/workspace
      - run:
          name: Copy SQL file to Azure Blob storage
          command: |
            azcopy \
              --source /tmp/workspace/cpdb_production_dump.sql.gz \
              --destination https://$PG_BACKUP_STORAGE_ACCOUNT_NAME.blob.core.windows.net/$PG_BACKUP_CONTAINER/cpdb_production_dump_$(date +%Y-%m-%d-%H:%M:%S).sql.gz \
              --dest-key $PG_BACKUP_STORAGE_ACCOUNT_KEY


workflows:
  version: 2
  test_backend:
    jobs:
      - test_backend:
          filters:
            branches:
              ignore:
                - staging
                - master
  deploy_backend_staging:
    jobs:
      - test_backend:
          filters:
            branches:
              only: staging
      - push_backend_image:
          requires:
            - test_backend
      - django_collect_static:
          requires:
            - push_backend_image
      - django_migrate:
          requires:
            - push_backend_image
      - deploy_backend:
          requires:
            - django_migrate
            - django_collect_static
  deploy_backend_production:
    jobs:
      - test_backend:
          filters:
            branches:
              only: master
      - push_backend_image:
          requires:
            - test_backend
      - django_collect_static:
          requires:
            - push_backend_image
      - django_migrate:
          requires:
            - push_backend_image
      - rebuild_index:
          requires:
            - django_migrate
      - rebuild_search_index:
          requires:
            - django_migrate
      - deploy_backend:
          requires:
            - rebuild_index
            - rebuild_search_index
            - django_collect_static

  daily_backup:
    triggers:
      - schedule:
          cron: "0 7 * * *"
          filters:
            branches:
              only:
                - master
    jobs:
      - create_sql_dump_file
      - upload_sql_file_to_azure:
          requires:
            - create_sql_dump_file

  deploy_cpdpbot:
    jobs:
      - build_cpdpbot:
          filters:
            branches:
              only:
                - staging
                - master
      - deploy_cpdpbot:
          filters:
            branches:
              only:
                - master

  deploy_ingress:
    jobs:
      - deploy_ingress:
          filters:
            branches:
              only:
                - staging
                - master
